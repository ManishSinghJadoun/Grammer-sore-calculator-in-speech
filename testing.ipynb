{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yth\n",
      "Pyth\n"
     ]
    }
   ],
   "source": [
    "s = \"Python\"\n",
    "print(s[1:4])  # \"yth\"\n",
    "print(s[:4])   # \"Pyth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (376514675.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(s[1::])class A:\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(s[1::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class B\n"
     ]
    }
   ],
   "source": [
    "class A:\n",
    "    def show(self):\n",
    "        print(\"Class A\")\n",
    "\n",
    "class B(A):\n",
    "    def show(self):\n",
    "        print(\"Class B\")\n",
    "\n",
    "class C(A):\n",
    "    def show(self):\n",
    "        print(\"Class C\")\n",
    "\n",
    "class D(B, C):  # Multiple Inheritance\n",
    "    pass\n",
    "\n",
    "obj = D()\n",
    "obj.show()  # Output: Class B (Left-to-right order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.D object at 0x000001729321E1B0>\n"
     ]
    }
   ],
   "source": [
    "print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8]\n",
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "nums = [1, 2, 3, 4]\n",
    "print(list(map(lambda x: x * 2, nums)))  # [2, 4, 6, 8]\n",
    "print(list(filter(lambda x: x % 2 == 0, nums)))  # [2, 4]\n",
    "  # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(reduce(lambda x, y: x + y, nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (2, 2)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "lst = [1, 2, 3, 1, 2, 1]\n",
    "print(Counter(lst).most_common(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grammar Scoring Engine - Multimodal (Audio + Grammar Features)\n",
    "\n",
    "# --- Brief Report ---\n",
    "\"\"\"\n",
    "Approach:\n",
    "We build a multimodal grammar scoring engine using spoken audio input. We extract features from two modalities:\n",
    "1. Acoustic features using Wav2Vec2\n",
    "2. Transcribed text features using Whisper and NLP techniques\n",
    "\n",
    "Preprocessing Steps:\n",
    "- Audio: Resample to 16kHz, pad/truncate to 60s\n",
    "- Text: Transcribe using Whisper, extract grammar-related features (POS tags, sentence counts, etc.)\n",
    "- Normalize grammar features using StandardScaler\n",
    "\n",
    "Model Architecture:\n",
    "- Wav2Vec2 (frozen) extracts audio embeddings\n",
    "- Grammar features pass through a small feedforward network\n",
    "- Combined and fed into a regressor\n",
    "\n",
    "Evaluation:\n",
    "- Pearson Correlation on validation set\n",
    "- Early stopping to prevent overfitting\n",
    "\n",
    "Tools: PyTorch, HuggingFace Transformers, Faster-Whisper, NLTK\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# --- Constants ---\n",
    "DATA_DIR = r\"D:\\shl-intern-hiring-assessment\\dataset\"\n",
    "AUDIO_DIR = os.path.join(DATA_DIR, \"audios_train\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"audios_test\")\n",
    "SAMPLE_RATE = 16000\n",
    "MAX_LENGTH = SAMPLE_RATE * 5  # 5 seconds\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# --- Grammar Feature Extractor ---\n",
    "def extract_grammar_features(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    num_tokens = len(tokens)\n",
    "    num_sentences = text.count('.') + text.count('!') + text.count('?')\n",
    "    num_nouns = len([tag for _, tag in pos_tags if tag.startswith('NN')])\n",
    "    num_verbs = len([tag for _, tag in pos_tags if tag.startswith('VB')])\n",
    "    num_adjs  = len([tag for _, tag in pos_tags if tag.startswith('JJ')])\n",
    "    return np.array([num_tokens, num_sentences, num_nouns, num_verbs, num_adjs])\n",
    "\n",
    "\n",
    "# --- Dataset ---\n",
    "class GrammarMultimodalDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, processor, whisper_model, scaler=None, fit_scaler=False):\n",
    "        self.df = df.copy()\n",
    "        self.audio_dir = audio_dir\n",
    "        self.processor = processor\n",
    "        self.whisper = whisper_model\n",
    "        self.scaler = scaler\n",
    "        self.fit_scaler = fit_scaler\n",
    "        self.grammar_features = []\n",
    "        self.transcripts = []\n",
    "\n",
    "        valid_rows = []\n",
    "        for i, file in enumerate(tqdm(df[\"filename\"], desc=\"Validating Audio\")):\n",
    "            audio_path = os.path.join(audio_dir, file)\n",
    "            try:\n",
    "                with sf.SoundFile(audio_path) as f:\n",
    "                    pass\n",
    "                valid_rows.append(i)\n",
    "            except Exception as e:\n",
    "                print(f\"[Skipping] {file}: {e}\")\n",
    "\n",
    "        self.df = self.df.iloc[valid_rows].reset_index(drop=True)\n",
    "        self.df = self.df.iloc[:4].reset_index(drop=True)  # For testing only\n",
    "\n",
    "        count = 0\n",
    "        for file in tqdm(self.df[\"filename\"], desc=\"Transcribing\"):\n",
    "            if count >= 4:\n",
    "                break\n",
    "\n",
    "            audio_path = os.path.join(audio_dir, file)\n",
    "            print(file)\n",
    "            feats = np.zeros(5)\n",
    "            text = \"\"\n",
    "\n",
    "            try:\n",
    "                segments, _ = self.whisper.transcribe(audio_path, beam_size=1)\n",
    "                print(segments)\n",
    "                text = \" \".join([seg.text for seg in segments])\n",
    "                print(text)\n",
    "                feats = extract_grammar_features(text)\n",
    "                print(feats)\n",
    "            except Exception as e:\n",
    "                print(f\"[Skipping] {file}: {e}\")\n",
    "                feats = np.zeros(5)\n",
    "                text = \"\"\n",
    "\n",
    "            self.grammar_features.append(feats)\n",
    "            self.transcripts.append(text)\n",
    "            del segments, feats\n",
    "            gc.collect()\n",
    "            count += 1\n",
    "\n",
    "        print(f\"[Info] Extracted grammar features for {len(self.grammar_features)} out of {len(self.df)} samples.\")\n",
    "        self.grammar_features = np.array(self.grammar_features)\n",
    "\n",
    "        if self.fit_scaler:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.grammar_features = self.scaler.fit_transform(self.grammar_features)\n",
    "        elif self.scaler:\n",
    "            self.grammar_features = self.scaler.transform(self.grammar_features)\n",
    "\n",
    "        self.df[\"transcription\"] = self.transcripts\n",
    "        print(self.df[[\"filename\", \"transcription\"]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = os.path.join(self.audio_dir, row[\"filename\"])\n",
    "\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(path)\n",
    "            waveform = torchaudio.functional.resample(waveform.mean(0), sr, SAMPLE_RATE)\n",
    "        except Exception as e:\n",
    "            print(f\"[Error loading audio {path}]: {e}\")\n",
    "            waveform = torch.zeros(MAX_LENGTH)\n",
    "\n",
    "        if len(waveform) > MAX_LENGTH:\n",
    "            waveform = waveform[:MAX_LENGTH]\n",
    "        else:\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, MAX_LENGTH - len(waveform)))\n",
    "\n",
    "        inputs = self.processor(waveform, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", return_attention_mask=True)\n",
    "        input_values = inputs.input_values[0]\n",
    "        attention_mask = inputs.attention_mask[0]\n",
    "        grammar_feats = torch.tensor(self.grammar_features[idx], dtype=torch.float)\n",
    "\n",
    "        if \"score\" in row:\n",
    "            label = torch.tensor(row[\"score\"], dtype=torch.float)\n",
    "            return input_values, attention_mask, grammar_feats, label\n",
    "        else:\n",
    "            return input_values, attention_mask, grammar_feats\n",
    "\n",
    "\n",
    "# --- Model ---\n",
    "class MultimodalGrammarModel(nn.Module):\n",
    "    def __init__(self, grammar_dim):\n",
    "        super().__init__()\n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc_audio = nn.Linear(self.wav2vec.config.hidden_size, 64)\n",
    "        self.fc_grammar = nn.Linear(grammar_dim, 32)\n",
    "        self.fc_combined = nn.Linear(96, 1)\n",
    "\n",
    "    def forward(self, input_values, attention_mask, grammar_feats):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.wav2vec(input_values, attention_mask=attention_mask)\n",
    "        audio_feat = outputs.last_hidden_state.mean(dim=1)\n",
    "        audio_feat = torch.relu(self.fc_audio(self.dropout(audio_feat)))\n",
    "        grammar_feat = torch.relu(self.fc_grammar(grammar_feats))\n",
    "        combined = torch.cat([audio_feat, grammar_feat], dim=1)\n",
    "        return self.fc_combined(combined).squeeze(1)\n",
    "\n",
    "\n",
    "# --- Training ---\n",
    "def train(model, train_loader, val_loader, epochs=10, lr=1e-4, patience=3):\n",
    "    model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "            if len(batch) == 4:\n",
    "                iv, am, gf, label = batch\n",
    "                iv, am, gf, label = iv.to(DEVICE), am.to(DEVICE), gf.to(DEVICE), label.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(iv, am, gf)\n",
    "                loss = criterion(output, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        val_losses, preds, targets = [], [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                if len(batch) == 4:\n",
    "                    iv, am, gf, label = batch\n",
    "                    iv, am, gf, label = iv.to(DEVICE), am.to(DEVICE), gf.to(DEVICE), label.to(DEVICE)\n",
    "                    output = model(iv, am, gf)\n",
    "                    val_losses.append(criterion(output, label).item())\n",
    "                    preds.extend(output.cpu().numpy())\n",
    "                    targets.extend(label.cpu().numpy())\n",
    "\n",
    "        if preds and targets:\n",
    "            val_loss = np.mean(val_losses)\n",
    "            val_pearson = pearsonr(preds, targets)[0]\n",
    "            print(f\"Epoch {epoch+1}: Train Loss={np.mean(train_losses):.4f}, Val Loss={val_loss:.4f}, Pearson={val_pearson:.4f}\")\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), \"best_model.pt\")\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping.\")\n",
    "                    break\n",
    "\n",
    "\n",
    "# --- Main ---\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
    "\n",
    "whisper_model = WhisperModel(\"base\", compute_type=\"int8\", device=\"cpu\")\n",
    "wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "train_dataset = GrammarMultimodalDataset(train_df, AUDIO_DIR, wav2vec_processor, whisper_model, fit_scaler=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = GrammarMultimodalDataset(train_df.iloc[:4], TEST_DIR, wav2vec_processor, whisper_model, scaler=train_dataset.scaler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, num_workers=0)\n",
    "\n",
    "model = MultimodalGrammarModel(grammar_dim=5)\n",
    "train(model, train_loader, val_loader)\n",
    "\n",
    "\n",
    "# --- Inference ---\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "def assign_grammar_grade(score):\n",
    "    if score < 1.5:\n",
    "        return 1\n",
    "    elif score < 2.5:\n",
    "        return 2\n",
    "    elif score < 3.5:\n",
    "        return 3\n",
    "    elif score < 4.5:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "def predict_and_save(model, test_df, test_audio_dir, processor, whisper_model, scaler):\n",
    "    model.eval()\n",
    "    test_dataset = GrammarMultimodalDataset(test_df, test_audio_dir, processor, whisper_model, scaler=scaler)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for iv, am, gf in tqdm(test_loader, desc=\"Predicting on test set\"):\n",
    "            iv, am, gf = iv.to(DEVICE), am.to(DEVICE), gf.to(DEVICE)\n",
    "            outputs = model(iv, am, gf)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "    test_df[\"score\"] = predictions\n",
    "    test_df[\"grade\"] = test_df[\"score\"].apply(assign_grammar_grade)\n",
    "    return test_df\n",
    "\n",
    "def predict_and_grade(dataloader, df):\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for iv, am, gf, _ in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            iv, am, gf = iv.to(DEVICE), am.to(DEVICE), gf.to(DEVICE)\n",
    "            output = model(iv, am, gf)\n",
    "            predictions.extend(output.cpu().numpy())\n",
    "\n",
    "    df[\"predicted_score\"] = predictions\n",
    "    df[\"predicted_grade\"] = df[\"predicted_score\"].apply(assign_grammar_grade)\n",
    "    return df\n",
    "\n",
    "# Apply grading\n",
    "val_df = predict_and_grade(val_loader, val_df)\n",
    "\n",
    "print(train_df[[\"filename\", \"score\", \"predicted_score\", \"predicted_grade\"]].head())\n",
    "print(val_df[[\"filename\", \"predicted_score\", \"predicted_grade\"]].head())\n",
    "\n",
    "# Final prediction on test set\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "final_val_df = predict_and_save(model, val_df, TEST_DIR, wav2vec_processor, whisper_model, train_dataset.scaler)\n",
    "\n",
    "# Save predictions\n",
    "final_val_df.to_csv(\"test_with_scores_and_grades.csv\", index=False)\n",
    "print(final_val_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# --- Constants ---\n",
    "DATA_DIR = r\"D:\\shl-intern-hiring-assessment\\dataset\"\n",
    "AUDIO_DIR = os.path.join(DATA_DIR, \"audios_train\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"audios_test\")\n",
    "SAMPLE_RATE = 16000\n",
    "MAX_LENGTH = SAMPLE_RATE * 5  # 5 seconds\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load Data ---\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "train_df.rename(columns={'label': 'score'}, inplace=True)\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
    "\n",
    "# --- Load Models ---\n",
    "whisper_model = WhisperModel(\"base\", compute_type=\"int8\", device=\"cpu\")\n",
    "wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# --- Grammar Feature Extractor ---\n",
    "def extract_grammar_features(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    num_tokens = len(tokens)\n",
    "    num_sentences = text.count('.') + text.count('!') + text.count('?')\n",
    "    num_nouns = len([tag for _, tag in pos_tags if tag.startswith('NN')])\n",
    "    num_verbs = len([tag for _, tag in pos_tags if tag.startswith('VB')])\n",
    "    num_adjs  = len([tag for _, tag in pos_tags if tag.startswith('JJ')])\n",
    "    return np.array([num_tokens, num_sentences, num_nouns, num_verbs, num_adjs])\n",
    "\n",
    "# --- Dataset ---\n",
    "class GrammarMultimodalDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, processor, whisper_model, scaler=None, fit_scaler=False):\n",
    "        self.df = df.copy()\n",
    "        self.audio_dir = audio_dir\n",
    "        self.processor = processor\n",
    "        self.whisper = whisper_model\n",
    "        self.scaler = scaler\n",
    "        self.fit_scaler = fit_scaler\n",
    "        self.grammar_features = []\n",
    "        self.transcripts = []\n",
    "\n",
    "        # Initialize grammar feature columns\n",
    "        for i in range(1, 6):\n",
    "            self.df[f\"grammar_feat_{i}\"] = np.nan\n",
    "        self.df[\"transcript\"] = \"\"\n",
    "\n",
    "        # Filter valid audio files\n",
    "        valid_rows = []\n",
    "        for i, file in enumerate(tqdm(self.df[\"filename\"], desc=\"Validating Audio\")):\n",
    "            audio_path = os.path.join(audio_dir, file)\n",
    "            try:\n",
    "                with sf.SoundFile(audio_path) as f:\n",
    "                    pass\n",
    "                valid_rows.append(i)\n",
    "            except Exception as e:\n",
    "                print(f\"[Skipping] {file}: {e}\")\n",
    "\n",
    "        self.df = self.df.iloc[valid_rows].reset_index(drop=True)\n",
    "        # self.df = self.df.iloc[:4].reset_index(drop=True)  # Optional: limit to 4 for speed\n",
    "\n",
    "        for idx, file in enumerate(tqdm(self.df[\"filename\"], desc=\"Transcribing\")):\n",
    "            audio_path = os.path.join(audio_dir, file)\n",
    "            print(f\"[Processing] {file}\")\n",
    "\n",
    "            feats = np.zeros(5)\n",
    "            text = \"\"\n",
    "\n",
    "            try:\n",
    "                segments, _ = self.whisper.transcribe(audio_path, beam_size=1)\n",
    "                if segments:\n",
    "                    text = \" \".join([seg.text for seg in segments])\n",
    "                    feats = extract_grammar_features(text)\n",
    "            except Exception as e:\n",
    "                print(f\"[Skipping] {file}: {e}\")\n",
    "\n",
    "            self.df.loc[idx, \"transcript\"] = text\n",
    "            for j, feat in enumerate(feats):\n",
    "                self.df.loc[idx, f\"grammar_feat_{j+1}\"] = feat\n",
    "\n",
    "            self.grammar_features.append(feats)\n",
    "            self.transcripts.append(text)\n",
    "\n",
    "            del feats\n",
    "            gc.collect()\n",
    "\n",
    "        print(f\"[Info] Extracted grammar features for {len(self.grammar_features)} samples.\")\n",
    "\n",
    "        if len(self.grammar_features) > 0:\n",
    "            self.grammar_features = np.array(self.grammar_features)\n",
    "            if self.fit_scaler:\n",
    "                self.scaler = StandardScaler()\n",
    "                self.grammar_features = self.scaler.fit_transform(self.grammar_features)\n",
    "            elif self.scaler:\n",
    "                self.grammar_features = self.scaler.transform(self.grammar_features)\n",
    "        else:\n",
    "            self.grammar_features = np.zeros((len(self.df), 5))  # fallback to zero features\n",
    "\n",
    "        self.df[\"transcription\"] = self.transcripts\n",
    "        print(self.df[[\"filename\", \"transcription\"]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = os.path.join(self.audio_dir, row[\"filename\"])\n",
    "\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(path)\n",
    "            waveform = torchaudio.functional.resample(waveform.mean(0), sr, SAMPLE_RATE)\n",
    "        except Exception as e:\n",
    "            print(f\"[Error loading audio {path}]: {e}\")\n",
    "            waveform = torch.zeros(MAX_LENGTH)\n",
    "\n",
    "        if len(waveform) > MAX_LENGTH:\n",
    "            waveform = waveform[:MAX_LENGTH]\n",
    "        else:\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, MAX_LENGTH - len(waveform)))\n",
    "\n",
    "        inputs = self.processor(waveform, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", return_attention_mask=True)\n",
    "        input_values = inputs.input_values[0]\n",
    "        attention_mask = inputs.attention_mask[0]\n",
    "        grammar_feats = torch.tensor(self.grammar_features[idx], dtype=torch.float)\n",
    "\n",
    "        if \"score\" in row:\n",
    "            label = torch.tensor(row[\"score\"], dtype=torch.float)\n",
    "            return input_values, attention_mask, grammar_feats, label\n",
    "        else:\n",
    "            return input_values, attention_mask, grammar_feats\n",
    "\n",
    "# --- Create Dataset & DataLoader ---\n",
    "train_dataset = GrammarMultimodalDataset(train_df, AUDIO_DIR, wav2vec_processor, whisper_model, fit_scaler=True)\n",
    "test_dataset = GrammarMultimodalDataset(test_df, TEST_DIR, wav2vec_processor, whisper_model, scaler=train_dataset.scaler)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Training: 100%|██████████| 222/222 [03:43<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 7.4143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 222/222 [03:36<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 1.6309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 222/222 [03:37<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 1.4437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 222/222 [03:35<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 1.3836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 222/222 [04:20<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 1.2945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training: 100%|██████████| 222/222 [04:10<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 1.2744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training: 100%|██████████| 222/222 [03:59<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 1.2872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Training: 100%|██████████| 222/222 [04:12<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 1.2119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Training: 100%|██████████| 222/222 [04:12<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 1.1919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Training: 100%|██████████| 222/222 [03:52<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 1.2158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Training: 100%|██████████| 222/222 [03:40<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss = 1.1885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Training: 100%|██████████| 222/222 [03:36<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss = 1.1684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Training: 100%|██████████| 222/222 [41:08<00:00, 11.12s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss = 1.2110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Training: 100%|██████████| 222/222 [05:59<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss = 1.1281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Training: 100%|██████████| 222/222 [05:46<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss = 1.1105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Training: 100%|██████████| 222/222 [04:57<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss = 1.1047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 Training: 100%|██████████| 222/222 [05:50<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss = 1.0616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 Training: 100%|██████████| 222/222 [05:05<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss = 1.0735\n",
      "✅ Final model saved to final_model.pt\n"
     ]
    }
   ],
   "source": [
    "# --- Model ---\n",
    "class MultimodalGrammarModel(nn.Module):\n",
    "    def __init__(self, grammar_dim):\n",
    "        super().__init__()\n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc_audio = nn.Linear(self.wav2vec.config.hidden_size, 64)\n",
    "        self.fc_grammar = nn.Linear(grammar_dim, 32)\n",
    "        self.fc_combined = nn.Linear(96, 1)\n",
    "\n",
    "    def forward(self, input_values, attention_mask, grammar_feats):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.wav2vec(input_values, attention_mask=attention_mask)\n",
    "        audio_feat = outputs.last_hidden_state.mean(dim=1)\n",
    "        audio_feat = torch.relu(self.fc_audio(self.dropout(audio_feat)))\n",
    "        grammar_feat = torch.relu(self.fc_grammar(grammar_feats))\n",
    "        combined = torch.cat([audio_feat, grammar_feat], dim=1)\n",
    "        return self.fc_combined(combined).squeeze(1)\n",
    "\n",
    "\n",
    "# --- Training ---\n",
    "def train(model, train_loader, epochs=18, lr=1e-4):\n",
    "    model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "            if len(batch) == 4:\n",
    "                iv, am, gf, label = batch\n",
    "                iv, am, gf, label = iv.to(DEVICE), am.to(DEVICE), gf.to(DEVICE), label.to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(iv, am, gf)\n",
    "                loss = criterion(output, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {np.mean(train_losses):.4f}\")\n",
    "\n",
    "    # ✅ Save model manually\n",
    "    torch.save(model.state_dict(), \"final_model.pt\")\n",
    "    print(\"✅ Final model saved to final_model.pt\")\n",
    "\n",
    "\n",
    "# --- Main ---\n",
    "\n",
    "model = MultimodalGrammarModel(grammar_dim=5)\n",
    "train(model, train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Predicting: 100%|██████████| 195/195 [01:55<00:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions to predicted_scores.csv\n",
      "         filename  predicted_score\n",
      "0   audio_706.wav         4.511966\n",
      "1   audio_800.wav         3.674064\n",
      "2    audio_68.wav         3.819784\n",
      "3  audio_1267.wav         4.079118\n",
      "4   audio_683.wav         3.620967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Model\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# --- Model Definition ---\n",
    "class MultimodalGrammarModel(nn.Module):\n",
    "    def __init__(self, grammar_dim):\n",
    "        super().__init__()\n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc_audio = nn.Linear(self.wav2vec.config.hidden_size, 64)\n",
    "        self.fc_grammar = nn.Linear(grammar_dim, 32)\n",
    "        self.fc_combined = nn.Linear(96, 1)\n",
    "\n",
    "    def forward(self, input_values, attention_mask, grammar_feats):\n",
    "        with torch.no_grad():  # freeze wav2vec\n",
    "            outputs = self.wav2vec(input_values, attention_mask=attention_mask)\n",
    "        audio_feat = outputs.last_hidden_state.mean(dim=1)\n",
    "        audio_feat = torch.relu(self.fc_audio(self.dropout(audio_feat)))\n",
    "        grammar_feat = torch.relu(self.fc_grammar(grammar_feats))\n",
    "        combined = torch.cat([audio_feat, grammar_feat], dim=1)\n",
    "        return self.fc_combined(combined).squeeze(1)\n",
    "\n",
    "# --- Load Model ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultimodalGrammarModel(grammar_dim=5)\n",
    "model.load_state_dict(torch.load(\"final_model.pt\", map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# --- Prediction ---\n",
    "predictions = []\n",
    "filenames = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_dataset)), desc=\"Predicting\"):\n",
    "        iv, am, gf = test_dataset[i]  # ✅ FIXED: Only 3 items returned\n",
    "        iv = iv.unsqueeze(0).to(DEVICE)          # add batch dim\n",
    "        am = am.unsqueeze(0).to(DEVICE)\n",
    "        gf = gf.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        output = model(iv, am, gf)\n",
    "        predictions.append(output.item())\n",
    "\n",
    "        # Get filename from dataset df\n",
    "        filename = test_dataset.df.iloc[i]['filename']\n",
    "        filenames.append(filename)\n",
    "\n",
    "# --- Save Predictions to CSV ---\n",
    "df_preds = pd.DataFrame({\n",
    "    \"filename\": filenames,\n",
    "    \"predicted_score\": predictions\n",
    "})\n",
    "df_preds.to_csv(\"predicted_scores.csv\", index=False)\n",
    "print(\"✅ Saved predictions to predicted_scores.csv\")\n",
    "print(df_preds.head())\n",
    "\n",
    "\n",
    "def assign_grammar_grade(score):\n",
    "    if score < 1.5:\n",
    "        return 1\n",
    "    elif score < 2.5:\n",
    "        return 2\n",
    "    elif score < 3.5:\n",
    "        return 3\n",
    "    elif score < 4.5:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "def predict_and_save(model, test_df, test_audio_dir, processor, whisper_model, scaler):\n",
    "    model.eval()\n",
    "    test_dataset = GrammarMultimodalDataset(test_df, test_audio_dir, processor, whisper_model, scaler=scaler)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for iv, am, gf in tqdm(test_loader, desc=\"Predicting on test set\"):\n",
    "            iv, am, gf = iv.to(DEVICE), am.to(DEVICE), gf.to(DEVICE)\n",
    "            outputs = model(iv, am, gf)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "    test_df[\"score\"] = predictions\n",
    "    test_df[\"grade\"] = test_df[\"score\"].apply(assign_grammar_grade)\n",
    "    return test_df\n",
    "\n",
    "\n",
    "\n",
    "print(train_df[[\"filename\", \"score\", \"predicted_score\", \"predicted_grade\"]].head())\n",
    "print(val_df[[\"filename\", \"predicted_score\", \"predicted_grade\"]].head())\n",
    "\n",
    "# Final prediction on test set\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "final_val_df = predict_and_save(model, val_df, TEST_DIR, wav2vec_processor, whisper_model, train_dataset.scaler)\n",
    "\n",
    "# Save predictions\n",
    "final_val_df.to_csv(\"test_with_scores_and_grades.csv\", index=False)\n",
    "print(final_val_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>transcript</th>\n",
       "      <th>grammar_feat_1</th>\n",
       "      <th>grammar_feat_2</th>\n",
       "      <th>grammar_feat_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audio_706.wav</td>\n",
       "      <td>レン カーニャ キェッ  サルデ ウラー サル レン  ディングッシュも なる  ディング...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audio_800.wav</td>\n",
       "      <td>My hobbies are playing cricket because I am a...</td>\n",
       "      <td>66.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audio_68.wav</td>\n",
       "      <td>At this market you can find a lot of things. ...</td>\n",
       "      <td>139.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audio_1267.wav</td>\n",
       "      <td>My goal is I become an interpreter in the pre...</td>\n",
       "      <td>78.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename                                         transcript  \\\n",
       "0   audio_706.wav   レン カーニャ キェッ  サルデ ウラー サル レン  ディングッシュも なる  ディング...   \n",
       "1   audio_800.wav   My hobbies are playing cricket because I am a...   \n",
       "2    audio_68.wav   At this market you can find a lot of things. ...   \n",
       "3  audio_1267.wav   My goal is I become an interpreter in the pre...   \n",
       "\n",
       "   grammar_feat_1  grammar_feat_2  grammar_feat_3  \n",
       "0            15.0             0.0            14.0  \n",
       "1            66.0             3.0            15.0  \n",
       "2           139.0             9.0            32.0  \n",
       "3            78.0             6.0            19.0  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_dataset.df[[\"filename\",\"transcript\", \"grammar_feat_1\", \"grammar_feat_2\", \"grammar_feat_3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audio_1261.wav</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audio_942.wav</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audio_1110.wav</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audio_1024.wav</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audio_538.wav</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>audio_494.wav</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>audio_363.wav</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>audio_481.wav</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>audio_989.wav</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>audio_1163.wav</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename  score\n",
       "0    audio_1261.wav    1.0\n",
       "1     audio_942.wav    1.5\n",
       "2    audio_1110.wav    1.5\n",
       "3    audio_1024.wav    1.5\n",
       "4     audio_538.wav    2.0\n",
       "..              ...    ...\n",
       "439   audio_494.wav    5.0\n",
       "440   audio_363.wav    5.0\n",
       "441   audio_481.wav    5.0\n",
       "442   audio_989.wav    5.0\n",
       "443  audio_1163.wav    5.0\n",
       "\n",
       "[444 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rename(columns={'label': 'score'}, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>score</th>\n",
       "      <th>new column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audio_1261.wav</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audio_942.wav</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audio_1110.wav</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audio_1024.wav</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audio_538.wav</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>audio_494.wav</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>audio_363.wav</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>audio_481.wav</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>audio_989.wav</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>audio_1163.wav</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename  score  new column\n",
       "0    audio_1261.wav    1.0           1\n",
       "1     audio_942.wav    1.5           1\n",
       "2    audio_1110.wav    1.5           1\n",
       "3    audio_1024.wav    1.5           1\n",
       "4     audio_538.wav    2.0           1\n",
       "..              ...    ...         ...\n",
       "439   audio_494.wav    5.0           1\n",
       "440   audio_363.wav    5.0           1\n",
       "441   audio_481.wav    5.0           1\n",
       "442   audio_989.wav    5.0           1\n",
       "443  audio_1163.wav    5.0           1\n",
       "\n",
       "[444 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"new column\"] = 1\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audio_1261.wav</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audio_942.wav</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audio_1110.wav</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audio_1024.wav</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audio_538.wav</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>audio_494.wav</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>audio_363.wav</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>audio_481.wav</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>audio_989.wav</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>audio_1163.wav</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename  score\n",
       "0    audio_1261.wav    1.0\n",
       "1     audio_942.wav    1.5\n",
       "2    audio_1110.wav    1.5\n",
       "3    audio_1024.wav    1.5\n",
       "4     audio_538.wav    2.0\n",
       "..              ...    ...\n",
       "439   audio_494.wav    5.0\n",
       "440   audio_363.wav    5.0\n",
       "441   audio_481.wav    5.0\n",
       "442   audio_989.wav    5.0\n",
       "443  audio_1163.wav    5.0\n",
       "\n",
       "[444 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop('new column', axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audio_942.wav</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audio_1110.wav</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audio_1024.wav</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audio_538.wav</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>audio_350.wav</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>audio_494.wav</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>audio_363.wav</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>audio_481.wav</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>audio_989.wav</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>audio_1163.wav</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>443 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename  score\n",
       "1     audio_942.wav    1.5\n",
       "2    audio_1110.wav    1.5\n",
       "3    audio_1024.wav    1.5\n",
       "4     audio_538.wav    2.0\n",
       "5     audio_350.wav    2.5\n",
       "..              ...    ...\n",
       "439   audio_494.wav    5.0\n",
       "440   audio_363.wav    5.0\n",
       "441   audio_481.wav    5.0\n",
       "442   audio_989.wav    5.0\n",
       "443  audio_1163.wav    5.0\n",
       "\n",
       "[443 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(index=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['filename', 'score'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['filename', 'label']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plot curves\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mtrain_losses_all\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(val_losses_all, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_losses_all' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot curves\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses_all, label=\"Train Loss\")\n",
    "plt.plot(val_losses_all, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(pearsons, label=\"Pearson\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.title(\"Validation Pearson\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(targets, preds, alpha=0.6)\n",
    "plt.plot([0, 5], [0, 5], '--r')\n",
    "plt.xlabel(\"True Score\")\n",
    "plt.ylabel(\"Predicted Score\")\n",
    "plt.title(\"Predicted vs Actual\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
